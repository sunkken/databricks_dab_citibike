# dab_project

The 'dab_project' project was generated by using the default-python template.

* `src/`: Python source code for this project.
* `resources/`:  Resource configurations (jobs, pipelines, etc.)
* `tests/`: Unit tests for the shared Python code.
* `fixtures/`: Fixtures for data sets (primarily used for testing).


## Getting started

Choose how you want to work on this project:

(a) Directly in your Databricks workspace, see
    https://docs.databricks.com/dev-tools/bundles/workspace.

(b) Locally with an IDE like Cursor or VS Code, see
    https://docs.databricks.com/dev-tools/vscode-ext.html.

(c) With command line tools, see https://docs.databricks.com/dev-tools/cli/databricks-cli.html

If you're developing with an IDE, dependencies for this project should be installed using uv:

*  Make sure you have the UV package manager installed.
   It's an alternative to tools like pip: https://docs.astral.sh/uv/getting-started/installation/.
*  To set up your Databricks Connect development environment, run:
   ```
   UV_PROJECT_ENVIRONMENT=.venv_dbc uv sync --group dbc --group dev
   ```
   This creates a `.venv_dbc` directory with all databricks connect environment dependencies defined in `pyproject.toml`.
   
*  To activate the environment:
   ```
   source .venv_dbc/bin/activate  # On macOS/Linux
   .venv_dbc\Scripts\activate    # On Windows
   ```

You can also use `UV_PROJECT_ENVIRONMENT=.venv_dbc uv run <command>` to run commands directly without activating the environment.

For a separate local PySpark environment, run:

```
UV_PROJECT_ENVIRONMENT=.venv_pyspark uv sync --no-dev --group pyspark
```

This creates `.venv_pyspark` with the local PySpark environment dependencies defined in `pyproject.toml`.
To activate it:

```
source .venv_pyspark/bin/activate  # On macOS/Linux
.venv_pyspark\Scripts\activate   # On Windows
```


# Using this project using the CLI

The Databricks workspace and IDE extensions provide a graphical interface for working
with this project. It's also possible to interact with it directly using the CLI:

1. Authenticate to your Databricks workspace, if you have not done so already:
    ```
    $ databricks configure
    ```

2. To deploy a development copy of this project, type:
    ```
    $ databricks bundle deploy --target dev
    ```
    (Note that "dev" is the default target, so the `--target` parameter
    is optional here.)

    This deploys everything that's defined for this project.

3. Similarly, to deploy a production copy, type:
   ```
   $ databricks bundle deploy --target prod
   ```

4. To run a job or pipeline, use the "run" command:
   ```
   $ databricks bundle run
   ```

5. Finally, to run tests locally, use `pytest`:
   ```
   $ uv run pytest
   ```
